#!/usr/bin/env python

import io
import logging
import math
import tempfile
from datetime import datetime
from os.path import join
from subprocess import Popen, PIPE, check_call

import click
from click_datetime import Datetime

from wagl.acquisition import acquisitions
from tesp.workflow import Package
from eodatasets.prepare.s2_prepare_cophub_zip import _process_datasets


DEFAULT_S2_AOI = '/g/data/v10/eoancillarydata/S2_extent/S2_aoi.csv'
DEFAULT_S2_L1C = '/g/data/fj7/Copernicus/Sentinel-2/MSI/L1C'
DEFAULT_WORKDIR = '/g/data/if87/datacube/002/S2_MSI_ARD/workdir'
DEFAULT_LOGDIR = '/g/data/if87/datacube/002/S2_MSI_ARD/log_dir'
DEFAULT_PKGDIR = '/g/data/if87/datacube/002/S2_MSI_ARD/packaged'

_NOW = datetime.now()


def _s2_date_filter(level1, start_date, end_date):
    """ Check if the processing DATE lies between start_date and end_date 

    >>> _s2_date_filter('S2A...R087_T57KXS_20180613T010330.zip', 
            datetime(2018, 06, 12), datetime(2018, 06, 14))
    True

    >>> _s2_date_filter('S2A...R087_T57KXS_20180613T010330.zip', 
            datetime(2018, 06, 10), datetime(2018, 06, 12))
    False

    """
    processing_date = datetime.strptime(
        level1.strip().rsplit('_', 1)[-1],
        '%Y%m%dT%H%M%S.zip'
    ).date()

    return (
        processing_date >= start_date.date() and 
        processing_date <= end_date.date()
    )


def _calc_nodes_req(granule_count, walltime, workers, hours_per_granule=1.5):
    """ Provides estimation of the number of nodes required to process granule count

    >>> _calc_nodes_req(400, '20:59', 28)
    2
    >>> _calc_nodes_req(800, '20:00', 28)
    3
    """

    hours, _, _ = [int(x) for x in walltime.split(':')]
    return int(math.ceil(float(hours_per_granule * granule_count) / (hours * workers)))


@click.group()
def cli():
    pass


@cli.command('process-level2')
@click.option('--level1-root', default=DEFAULT_S2_L1C, type=str,
              help="Folder containing Sentinel-2 level-1 datasets.")
@click.option('--s2-aoi', default=DEFAULT_S2_AOI, type=str,
              help="List of MGRD tiles of interest.")
@click.option('--start-date', type=Datetime(format='%Y-%m-%d'),
              help="Start of date range to process.")
@click.option('--end-date', type=Datetime(format='%Y-%m-%d'),
              help="End of date range to process.")
@click.option('--pkgdir', default=DEFAULT_PKGDIR, type=click.Path(file_okay=False, writable=True),
              help="The base output packaged directory.")
@click.option("--workdir", default=DEFAULT_WORKDIR, type=click.Path(file_okay=False, writable=True),
              help="The base output working directory.")
@click.option("--logdir", default=DEFAULT_LOGDIR, type=click.Path(file_okay=False, writable=True),
              help="The base logging and scripts output directory.")
@click.option("--env", type=click.Path(exists=True, readable=True),
              help="Environment script to source.")
@click.option("--workers", type=click.IntRange(1, 32), default=28,
              help="The number of workers to request per node.")
@click.option("--memory", default=256,
              help="The memory in GB to request per node.")
@click.option("--jobfs", default=60,
              help="The jobfs memory in GB to request per node.")
@click.option("--project", required=True, help="Project code to run under.")
@click.option("--queue", default='normalbw',
              help="Queue to submit the job into, e.g. normalbw, expressbw.")
@click.option("--walltime", default="48:00:00",
              help="Job walltime in `hh:mm:ss` format.")
@click.option("--email", default="your.name@something.com",
              help="Notification email address.")
@click.option("--test", default=False, is_flag=True,
              help="Test job execution (Don't submit the job to the PBS queue).")
def process_level2(level1_root, s2_aoi, start_date, end_date, pkgdir, workdir, logdir, env,
         workers, memory, jobfs, project, queue, walltime, email, test):
    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s', level=logging.INFO)

    # Read area of interest list
    with open(s2_aoi) as csv:
        tile_ids = {'T' + tile.strip() for tile in csv}

    def date(date_time):
        return date_time.strftime('%Y-%m-%d')

    if start_date is None and end_date is None:
        def date_filter(line):
            return True
    else:
        date_filter = functools.partial(
            _s2_date_filter,
            start_date=datetime(_NOW.year, 1, 1),
            end_date=_NOW
        )

    logging.info("calling %s", ' '.join(cmd))
    in_stream = io.TextIOWrapper(Popen(cmd, stdout=PIPE).stdout, encoding='utf-8')

    def filter_granule_worker(in_stream, out_stream):
        count = 0

        for line in in_stream:
            level1 = line.strip()

            if not date_filter(level1):
                logging.info('skipping %s because of date filter', level1)
                continue

            try:
                container = acquisitions(level1)
            except Exception as e:
                logging.warning('encountered unexpected error for %s: %s', level1, e)
                continue

            for granule in container.granules:
                # does it contain any granules that need to be processed?
                acq = container.get_acquisitions(None, granule, False)[0]
                ymd = acq.acquisition_datetime.strftime('%Y-%m-%d')
                tile_id = granule.split('_')[-2]
                package = Package(level1, '', granule, join(pkgdir, ymd))

                if tile_id not in tile_ids:
                    logging.info('granule %s with MGRS tile ID %s outside AOI', granule, tile_id)
                elif package.output().exists():
                    logging.info('granule %s already processed', granule)
                else:
                    # yes it does
                    logging.info('level1 dataset %s needs to be processed', level1)
                    print(level1, file=out_stream)
                    # a more realistic count increment would be len(container.granules)
                    count += 1
                    break

        return outstream, count

    with tempfile.NamedTemporaryFile(mode="w+") as out_stream:
        _, granule_count = filter_granule_worker(in_stream, out_stream)
        out_stream.flush()

        if granule_count == 0:
            logging.info("no granules to process.")
            return

        num_nodes = _calc_nodes_req(granule_count, walltime, workers)
        assert num_nodes > 0, "cannot ask for {} nodes".format(num_nodes)
        assert num_nodes <= 200, "number of nodes to request {} is too high".format(num_nodes)

        next_cmd = ['ard_pbs', '--level1-list', out_stream.name, '--workdir', workdir,
                    '--logdir', logdir, '--pkgdir', pkgdir, '--env', env,
                    '--workers', str(workers), '--nodes', str(num_nodes), '--memory', str(memory),
                    '--jobfs', str(jobfs), '--project', project, '--queue', queue, '--walltime', walltime,
                    '--email', email] + (['--test'] if test else [])

        logging.info("calling %s", ' '.join(next_cmd))
        check_call(next_cmd)


@click.command('generate-level1')
@click.option('--output', 'output_dir',
              type=click.Path(exists=True, writable=True)
              help='directory to write yamls to')
@click.option('--start-date', type=Datetime('%Y-%m-%d'),
              default=(_NOW.year, 1, 1),
              help='Start of date range for level 1 generation')
@click.option('--end-date', type=Datetime('%Y-%m-%d'),
              default=_NOW,
              help='End of date range for level 1 generation')
@click.option('--checksum/--no-checksum', default=False)
def generate_level1(output_dir:Path, start_date:datetime, end_date:datetime, checksum:bool):
    # run a find command to generate a list of level1 documents
    cmd = ['find', level1_root, '-name', '*.zip']
    logging.info("calling %s", ' '.join(cmd))
    in_stream = io.TextIOWrapper(Popen(cmd, stdout=PIPE).stdout, encoding='utf-8')
    for level1_archive in in_stream:
        level1_archive = level1_archive.strip()
        if _s2_date_filter(level1_archive, start_date, end_date):
            _process_datasets(output_dir, (level1_archive, ), checksum, start_date)


if __name__ == '__main__':
    cli()
